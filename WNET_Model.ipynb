{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJEVgMfAgmuxRui+ZeIZ/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moeghaf/Deep-unsupervised-segmentation/blob/main/WNET_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WNET - Deep unsupervised segmentation\n",
        "\n",
        "Model implementation of WNet -  https://arxiv.org/pdf/1711.08506.pdf\n",
        "\n",
        "Config, data loader and NCut adapted from: https://github.com/Andrew-booler/W-Net/tree/master/Wnet\n"
      ],
      "metadata": {
        "id": "W3IN7jFb9x5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torch.utils.data as Data\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pdb\n",
        "import math\n",
        "import cupy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io, color, morphology\n",
        "from torchvision import transforms as T\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.dpi'] = 300"
      ],
      "metadata": {
        "id": "faSh3euF-ob3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "\n",
        "    def __init__(self):\n",
        "        #network configure\n",
        "        self.InputCh=3\n",
        "        self.ScaleRatio = 2\n",
        "        self.ConvSize = 3\n",
        "        self.pad = 1#(self.ConvSize - 1) / 2\n",
        "        self.MaxLv = 5\n",
        "        self.ChNum = [self.InputCh,64]\n",
        "        for i in range(self.MaxLv-1):\n",
        "            self.ChNum.append(self.ChNum[-1]*2)\n",
        "\n",
        "        #self.imagelist = \"ImageSets/Segmentation/train.txt\"\n",
        "        self.BatchSize = 16\n",
        "        self.Shuffle = True\n",
        "        self.LoadThread = 2\n",
        "        self.inputsize = [224,224]\n",
        "        #self.CollagenDir = '/content/drive/MyDrive/Studies/PhD_WT_IMC/Year_2 /WnET/clean_col_data/*tiff'\n",
        "        #self.CollagenDir = '/content/drive/MyDrive/Studies/PhD_WT_IMC/Year_2 /WnET/clean_col_data_not_mask/*tiff'\n",
        "        #self.CollagenDir = '/content/drive/MyDrive/Studies/PhD_WT_IMC/Year_2 /WnET/raw_col_data/*tiff'\n",
        "        #self.CollagenDir = '/content/drive/MyDrive/Others/WnET/raw_col_data/*tiff'\n",
        "\n",
        "        #partition configure\n",
        "        self.K = 10\n",
        "\n",
        "        #training configure\n",
        "        self.init_lr = 0.05\n",
        "        self.lr_decay = 0.1\n",
        "        self.lr_decay_iter = 1000\n",
        "        self.max_iter = 50000\n",
        "        self.cuda_dev = 0\n",
        "        self.cuda_dev_list = \"0,1\"\n",
        "        self.check_iter = 1000\n",
        "\n",
        "        #Ncuts Loss configure\n",
        "        self.radius = 2\n",
        "        self.sigmaI = 10\n",
        "        self.sigmaX = 1\n",
        "\n",
        "\n",
        "        #color library\n",
        "        self.color_lib = []\n",
        "        #color library\n",
        "        self.color_lib = []\n",
        "        for r in range(0,256,128):\n",
        "            for g in range(0,256,128):\n",
        "                for b in range(0,256,128):\n",
        "                    self.color_lib.append((r,g,b))\n",
        "\n",
        "\n",
        "\n",
        "class DataLoader():\n",
        "    #initialization\n",
        "\n",
        "    def __init__(self, datapath,mode):\n",
        "        self.raw_data = []\n",
        "        self.mode = mode\n",
        "\n",
        "        datapath = glob.glob(datapath)\n",
        "\n",
        "        #load the images\n",
        "        for file_name in datapath:\n",
        "            with Image.open(file_name) as image:\n",
        "\n",
        "                # MODIFIED\n",
        "                image = np.array(image) - 1\n",
        "                image = Image.fromarray(image)\n",
        "                if image.mode != \"RGB\":\n",
        "                    image = image.convert(\"RGB\")\n",
        "                self.raw_data.append(np.array(image.resize((config.inputsize[0],config.inputsize[1]),Image.BILINEAR)))\n",
        "        #self.raw_data = np.array(self.raw_data)\n",
        "\n",
        "        # Data augment\n",
        "\n",
        "        #resize and align\n",
        "        self.scale()\n",
        "        #normalize\n",
        "        self.transfer()\n",
        "\n",
        "        #calculate weights by 2\n",
        "        if(mode == \"train\"):\n",
        "            self.dataset = self.get_dataset(self.raw_data, self.raw_data.shape,75)\n",
        "        else:\n",
        "            self.dataset = self.get_dataset(self.raw_data, self.raw_data.shape,75)\n",
        "\n",
        "    def scale(self):\n",
        "        for i in range(len(self.raw_data)):\n",
        "            image = self.raw_data[i]\n",
        "            self.raw_data[i] = np.stack((image[:,:,0],image[:,:,1],image[:,:,2]),axis = 0)\n",
        "        self.raw_data = np.stack(self.raw_data,axis = 0)\n",
        "\n",
        "    def transfer(self):\n",
        "        #just for RGB 8-bit color\n",
        "        self.raw_data = self.raw_data.astype(float)\n",
        "        #for i in range(self.raw_data.shape[0]):\n",
        "        #    Image.fromarray(self.raw_data[i].swapaxes(0,-1).astype(np.uint8)).save(\"./reconstruction/input_\"+str(i)+\".jpg\")\n",
        "\n",
        "    def torch_loader(self):\n",
        "        return Data.DataLoader(\n",
        "                                self.dataset,\n",
        "                                batch_size = config.BatchSize,\n",
        "                                shuffle = config.Shuffle,\n",
        "                                num_workers = config.LoadThread,\n",
        "                                pin_memory = True,\n",
        "                            )\n",
        "\n",
        "    def cal_weight(self,raw_data,shape):\n",
        "        print(\"calculating weights.\")\n",
        "\n",
        "        dissim = cp.zeros((shape[0],shape[1],shape[2],shape[3],(config.radius-1)*2+1,(config.radius-1)*2+1))\n",
        "        data = cp.asarray(raw_data)\n",
        "        padded_data = cp.pad(data,((0,0),(0,0),(config.radius-1,config.radius-1),(config.radius-1,config.radius-1)),'constant')\n",
        "        for m in range(2*(config.radius-1)+1):\n",
        "            for n in range(2*(config.radius-1)+1):\n",
        "                dissim[:,:,:,:,m,n] = data-padded_data[:,:,m:shape[2]+m,n:shape[3]+n]\n",
        "        #for i in range(dissim.shape[0]):\n",
        "        #dissim = -cp.power(dissim,2).sum(1,keepdims = True)/config.sigmaI/config.sigmaI\n",
        "        temp_dissim = cp.exp(-cp.power(dissim,2).sum(1,keepdims = True)/config.sigmaI**2)\n",
        "        dist = cp.zeros((2*(config.radius-1)+1,2*(config.radius-1)+1))\n",
        "        for m in range(1-config.radius,config.radius):\n",
        "            for n in range(1-config.radius,config.radius):\n",
        "                if m**2+n**2<config.radius**2:\n",
        "                    dist[m+config.radius-1,n+config.radius-1] = cp.exp(-(m**2+n**2)/config.sigmaX**2)\n",
        "\n",
        "\n",
        "        print(\"weight calculated.\")\n",
        "        res = cp.multiply(temp_dissim,dist)\n",
        "\n",
        "        return res\n",
        "\n",
        "    def get_dataset(self,raw_data,shape,batch_size):\n",
        "        dataset = []\n",
        "        for batch_id in range(0,shape[0],batch_size):\n",
        "            print(batch_id)\n",
        "            batch = raw_data[batch_id:min(shape[0],batch_id+batch_size)]\n",
        "            if(self.mode == \"train\"):\n",
        "                tmp_weight = self.cal_weight(batch,batch.shape)\n",
        "                weight = cp.asnumpy(tmp_weight)\n",
        "                dataset.append(Data.TensorDataset(torch.from_numpy(batch).float(),torch.from_numpy(weight).float()))\n",
        "                del tmp_weight\n",
        "            else:\n",
        "                dataset.append(Data.TensorDataset(torch.from_numpy(batch).float()))\n",
        "        cp.get_default_memory_pool().free_all_blocks()\n",
        "        return Data.ConcatDataset(dataset)\n",
        "\n",
        "config = Config()\n",
        "\n",
        "\n",
        "# Build encoder and test training\n",
        "# Implementation from https://arxiv.org/pdf/1711.08506.pdf\n",
        "# Modified to weigh the Ncut loss and reconstruction loss\n",
        "import torch.nn as nn\n",
        "\n",
        "def add_conv(in_ch, out_ch):\n",
        "  return torch.nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding = 1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(out_ch),\n",
        "\n",
        "                                    nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding = 1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(out_ch))\n",
        "\n",
        "def add_sep_conv(in_ch,out_ch):\n",
        "  ''' defined as a depthwise and point wise convolution '''\n",
        "  return torch.nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=1),\n",
        "                                    nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1, groups=out_ch),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(out_ch),\n",
        "\n",
        "                                    nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=1),\n",
        "                                    nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1, groups=out_ch),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(out_ch))\n",
        "\n",
        "\n",
        "class WNetEncoder(nn.Module):\n",
        "    ''' WNet '''\n",
        "    def __init__(self, k=config.K, radius=config.radius):\n",
        "        super(WNetEncoder, self).__init__()\n",
        "        self.radius = radius\n",
        "        self.k = k\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.pad = nn.ConstantPad2d(4,0)\n",
        "\n",
        "\n",
        "        # Module 1 - Conv 3x3\n",
        "        self.module1 = add_conv(3, 64)\n",
        "\n",
        "\n",
        "        # Module2 - separable conv 3x3,\n",
        "        self.module2 = add_sep_conv(64 ,128)\n",
        "\n",
        "        # Module3 - separable conv 3x3\n",
        "        self.module3 =  add_sep_conv(128 , 256)\n",
        "\n",
        "        # Module 4 - separable conv 3x3\n",
        "        self.module4 = add_sep_conv(256 , 512)\n",
        "\n",
        "        # Module 5 - separable conv 3x3 and upconv\n",
        "        self.module5 = add_sep_conv(512 , 1024)\n",
        "        self.upconv5 = nn.ConvTranspose2d(1024, 512, 2,2)\n",
        "\n",
        "        # Module 6 - separable conv 3x3 and upconv\n",
        "        self.module6 = add_sep_conv(1024 , 512)\n",
        "        self.upconv6 = nn.ConvTranspose2d(512, 256, 2,2)\n",
        "\n",
        "        # Module 7 - separable conv 3x3 and upconv\n",
        "        self.module7 = add_sep_conv(512 , 256)\n",
        "        self.upconv7 = nn.ConvTranspose2d(256, 128, 2,2)\n",
        "\n",
        "        # Module 8 - separable conv 3x3 and upconv\n",
        "        self.module8 = add_sep_conv(256 , 128)\n",
        "        self.upconv8 = nn.ConvTranspose2d(128, 64, 2,2)\n",
        "\n",
        "        # Module 9\n",
        "        self.module9 = add_conv(128, 64)\n",
        "        self.predconv = nn.Conv2d(64,self.k,1)\n",
        "        self.softmax = nn.Softmax2d()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "      # Module 1 and maxpool\n",
        "      x1_skip = self.module1(x)\n",
        "      x1 = self.maxpool(x1_skip)\n",
        "\n",
        "\n",
        "      # Module 2 and maxpool\n",
        "      x2_skip = self.module2(x1)\n",
        "      x2 = self.maxpool(x2_skip)\n",
        "\n",
        "      # Module 3 and maxpool\n",
        "      x3_skip = self.module3(x2)\n",
        "      x3 = self.maxpool(x3_skip)\n",
        "\n",
        "      # Module 4 and maxpool\n",
        "      x4_skip = self.module4(x3) # Join with output of module 5\n",
        "      x4 = self.maxpool(x4_skip)\n",
        "\n",
        "      # Module 5 and maxpool\n",
        "      x5 = self.module5(x4)\n",
        "      x5 = self.upconv5(x5)\n",
        "\n",
        "      # Skip and module 6\n",
        "      skip1 = torch.concat((x5,x4_skip), dim=1)\n",
        "      x6 = self.module6(skip1)\n",
        "      x6 = self.upconv6(x6)\n",
        "\n",
        "      # Module 7\n",
        "      skip2 = torch.concat((x6, x3_skip), dim=1)\n",
        "      x7 = self.module7(skip2)\n",
        "      x7 = self.upconv7(x7)\n",
        "\n",
        "      # Module 8\n",
        "      skip3 = torch.concat((x7, x2_skip), dim=1)\n",
        "      x8 = self.module8(skip3)\n",
        "      x8 = self.upconv8(x8)\n",
        "\n",
        "      # Module 9\n",
        "      skip4 = torch.concat((x8, x1_skip), dim=1)\n",
        "      x9 = self.module9(skip4)\n",
        "      x9 = self.predconv(x9)\n",
        "      self.k_pred = self.softmax(x9)\n",
        "\n",
        "      return self.k_pred, self.pad(self.k_pred)\n",
        "\n",
        "\n",
        "class WNetDecoder(nn.Module):\n",
        "  def __init__(self, k=config.K):\n",
        "    super(WNetDecoder, self).__init__()\n",
        "    self.k = k\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "\n",
        "    # Module 10\n",
        "    self.module10 = add_conv(self.k, 64)\n",
        "\n",
        "    # Module 11\n",
        "    self.module11 = add_sep_conv(64, 128)\n",
        "\n",
        "    # Module 12\n",
        "    self.module12 = add_sep_conv(128, 256)\n",
        "\n",
        "    # Module 13\n",
        "    self.module13 = add_sep_conv(256, 512)\n",
        "\n",
        "    # Module 14\n",
        "    self.module14 = add_sep_conv(512, 1024)\n",
        "    self.module14.append(nn.ConvTranspose2d(1024, 512, 2,2))\n",
        "\n",
        "    # Module 15\n",
        "    self.module15 = add_sep_conv(1024, 512)\n",
        "    self.module15.append(nn.ConvTranspose2d(512, 256, 2,2))\n",
        "\n",
        "    # Module 16\n",
        "    self.module16 = add_sep_conv(512, 256)\n",
        "    self.module16.append(nn.ConvTranspose2d(256, 128, 2,2))\n",
        "\n",
        "    # Module 17\n",
        "    self.module17 = add_sep_conv(256, 128)\n",
        "    self.module17.append(nn.ConvTranspose2d(128, 64, 2,2))\n",
        "\n",
        "    # Module 18\n",
        "    self.module18 = add_conv(128, 64)\n",
        "    self.module18.append(nn.Conv2d(64,3 ,1))\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    # Module 10 and maxpool, x10_skip concat with module 18\n",
        "    x10_skip = self.module10(x)\n",
        "    x10 = self.maxpool(x10_skip)\n",
        "\n",
        "    # Module 11 and maxpool\n",
        "    x11_skip = self.module11(x10)\n",
        "    x11 = self.maxpool(x11_skip)\n",
        "\n",
        "    # Module 12 and maxpool\n",
        "    x12_skip = self.module12(x11)\n",
        "    x12 = self.maxpool(x12_skip)\n",
        "\n",
        "    # Module 13 and maxpool\n",
        "    x13_skip = self.module13(x12)\n",
        "    x13 = self.maxpool(x13_skip)\n",
        "\n",
        "    # Module 14 and upconv\n",
        "    x14 = self.module14(x13)\n",
        "\n",
        "    # Module 15, skip connection from 13\n",
        "    skip13_to_15 =  torch.concat((x13_skip, x14), dim=1)\n",
        "    x15 = self.module15(skip13_to_15)\n",
        "\n",
        "    # Module 16, skip connection from 12\n",
        "    skip12_to_16 = torch.concat((x12_skip, x15), dim=1)\n",
        "    x16 = self.module16(skip12_to_16)\n",
        "\n",
        "    # Module 17, skip connection from 11\n",
        "    skip11_to_17 = torch.concat((x11_skip, x16),dim=1)\n",
        "    x17 = self.module17(skip11_to_17)\n",
        "\n",
        "    # Module 18, skip connection from 10\n",
        "    skip10_to_18 = torch.concat((x10_skip, x17), dim=1)\n",
        "    x_pred = self.module18(skip10_to_18)\n",
        "    return x_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class WNet(nn.Module):\n",
        "  def __init__(self, K= config.K):\n",
        "    super(WNet, self).__init__()\n",
        "\n",
        "    self.K = K\n",
        "    self.encoder = WNetEncoder(self.K)\n",
        "    self.decoder = WNetDecoder(self.K)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x_segmented, xseg_pad = self.encoder(x)\n",
        "    x_pred = self.decoder(x_segmented)\n",
        "    return x_pred, x_segmented, xseg_pad\n",
        "\n",
        "\n",
        "\n",
        "class NCutsLoss(nn.Module):\n",
        "    def __init__(self, radius=config.radius):\n",
        "        super(NCutsLoss,self).__init__()\n",
        "        self.radius = radius\n",
        "        self.gpu_list = []\n",
        "\n",
        "    def forward(self, seg, padded_seg, weight,sum_weight):\n",
        "        cropped_seg = []\n",
        "        for m in torch.arange((self.radius-1)*2+1,dtype=torch.long):\n",
        "            column = []\n",
        "            for n in torch.arange((self.radius-1)*2+1,dtype=torch.long):\n",
        "                column.append(padded_seg[:,:,m:m+seg.size()[2],n:n+seg.size()[3]].clone())\n",
        "            cropped_seg.append(torch.stack(column,4))\n",
        "        cropped_seg = torch.stack(cropped_seg,4)\n",
        "\n",
        "        multi1 = cropped_seg.mul(weight)\n",
        "        multi2 = multi1.sum(-1).sum(-1).mul(seg)\n",
        "        multi3 = sum_weight.mul(seg)\n",
        "\n",
        "        assocA = multi2.view(multi2.shape[0],multi2.shape[1],-1).sum(-1)\n",
        "        assocV = multi3.view(multi3.shape[0],multi3.shape[1],-1).sum(-1)\n",
        "        assoc = assocA.div(assocV).sum(-1)\n",
        "\n",
        "        return torch.add(-assoc,10)\n",
        "\n"
      ],
      "metadata": {
        "id": "VJyBtlTC-NTn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}